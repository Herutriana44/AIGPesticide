{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 17:44:42.255703: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-21 17:44:42.256323: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-21 17:44:42.256386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (debian): /proc/driver/nvidia/version does not exist\n",
      "2023-03-21 17:44:42.257301: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/herutriana44/.local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 3s 71ms/step - loss: 0.6935 - accuracy: 0.4759\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.6604 - accuracy: 0.8026\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.6022 - accuracy: 0.7675\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4874 - accuracy: 0.8311\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.3862 - accuracy: 0.8443\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 0.3053 - accuracy: 0.8662\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.2371 - accuracy: 0.9079\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.1945 - accuracy: 0.9364\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 1s 58ms/step - loss: 0.1588 - accuracy: 0.9474\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.1250 - accuracy: 0.9671\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.5731 - accuracy: 0.5614\n",
      "Test loss: 1.5731362104415894\n",
      "Test accuracy: 0.5614035129547119\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('ActiveCompoundPersticide.csv')\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "data['Status'] = le.fit_transform(data['Status'])\n",
    "\n",
    "# Transform features with one hot encoding\n",
    "one_hot = pd.get_dummies(data['Substance'])\n",
    "X = one_hot.values.reshape(len(data), one_hot.shape[1], 1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data['Status'], test_size=0.2)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the CNN model\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the CNN model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......conv1d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......max_pooling1d\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "variables.h5                                   2023-03-21 17:47:38      7062456\n",
      "config.json                                    2023-03-21 17:47:38         2542\n",
      "metadata.json                                  2023-03-21 17:47:38           64\n"
     ]
    }
   ],
   "source": [
    "# save model ke pickle\n",
    "import pickle\n",
    "pickle.dump(model, open('CNNAIPesticide.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pickle model\n",
    "import pickle\n",
    "model = pickle.load(open('CNNAIPesticide.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show CNN architecture model in here use matplotlib\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Membangun model generator\n",
    "generator = Sequential()\n",
    "generator.add(Dense(256, input_dim=100, kernel_initializer='glorot_normal', activation='relu'))\n",
    "generator.add(Dense(512, activation='relu'))\n",
    "generator.add(Dense(1024, activation='relu'))\n",
    "generator.add(Dense(2048, activation='relu'))\n",
    "generator.add(Dense(4096, activation='relu'))\n",
    "generator.add(Dense(8192, activation='relu'))\n",
    "generator.add(Dense(10912, activation='softmax'))\n",
    "\n",
    "# Menyimpan model ke file pickle\n",
    "with open('generator_model.pkl', 'wb') as file:\n",
    "    pickle.dump(generator, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show generator architecture model in here use matplotlib\n",
    "from keras.utils import plot_model\n",
    "plot_model(generator, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random smiles data using generator model\n",
    "import pickle\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "# load model\n",
    "with open('generator_model.pkl', 'rb') as file:\n",
    "    generator = pickle.load(file)\n",
    "\n",
    "# generate random smiles data\n",
    "smiles = []\n",
    "for i in range(10):\n",
    "    z = np.random.normal(0, 1, (1, 100))\n",
    "    generated_smiles = generator.predict(z)\n",
    "    generated_smiles = generated_smiles[0]\n",
    "    generated_smiles = np.argmax(generated_smiles)\n",
    "    generated_smiles = np.binary_repr(generated_smiles, width=10912)\n",
    "    print(generated_smiles)\n",
    "    generated_smiles = Chem.MolFromSmiles(generated_smiles)\n",
    "    smiles.append(generated_smiles)\n",
    "\n",
    "# print generated smiles data\n",
    "print(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# membaca dataset\n",
    "data = pd.read_csv('ActiveCompoundPersticide.csv')\n",
    "\n",
    "# mengubah kolom Substance menjadi sebuah list\n",
    "smiles_list = data['Substance'].tolist()\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def smiles_to_vector(smiles, max_length=100):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n",
    "    arr = np.zeros((1,))\n",
    "    Chem.DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "    return pad_sequences([arr], maxlen=max_length, dtype='float32', padding='post')\n",
    "\n",
    "def generate_random_input_vector(n_samples, input_dim=100):\n",
    "    return np.random.normal(size=(n_samples, input_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latih model generator\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# konfigurasi hyperparameter\n",
    "batch_size = 128\n",
    "epochs = 10000\n",
    "input_dim = 100\n",
    "\n",
    "# kompilasi model\n",
    "generator.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "\n",
    "# latih model\n",
    "for epoch in range(epochs):\n",
    "    # generate data acak\n",
    "    random_input = generate_random_input_vector(batch_size, input_dim)\n",
    "    # generate data SMILES dari data acak\n",
    "    generated_smiles = generator.predict(random_input)\n",
    "    # latih model dengan data SMILES yang telah digenerate\n",
    "    generator.train_on_batch(random_input, generated_smiles)\n",
    "    # tampilkan progress\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch} / {epochs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# simpan model generator ke dalam file pickle\n",
    "with open('generator_model.pickle', 'wb') as f:\n",
    "    pickle.dump(generator, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 00:40:33.811103: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-26 00:40:34.464651: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-26 00:40:34.464676: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-26 00:40:36.030938: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-26 00:40:36.031108: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-26 00:40:36.031123: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-26 00:40:37.669894: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-26 00:40:37.670365: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-26 00:40:37.670394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (debian): /proc/driver/nvidia/version does not exist\n",
      "2023-03-26 00:40:37.671738: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-26 00:40:38.103792: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 6710886400 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# hapus warning \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('ActiveCompoundPersticide.csv')\n",
    "\n",
    "# Get the SMILES data\n",
    "smiles_list = data['Substance'].tolist()\n",
    "\n",
    "# Define constants\n",
    "max_length = 100\n",
    "input_dim = 2048\n",
    "\n",
    "# Define the generator model\n",
    "generator = Sequential()\n",
    "generator.add(Dense(256, input_dim=input_dim, activation='relu'))\n",
    "generator.add(Dense(512, activation='relu'))\n",
    "generator.add(Dense(1024, activation='relu'))\n",
    "generator.add(Dense(2048, activation='relu'))\n",
    "generator.add(Dense(4096, activation='relu'))\n",
    "generator.add(Dense(8192, activation='relu'))\n",
    "generator.add(Dense(max_length*input_dim, activation='relu'))\n",
    "generator.add(Reshape((max_length, input_dim)))\n",
    "generator.add(LSTM(512, return_sequences=True))\n",
    "generator.add(Dropout(0.2))\n",
    "generator.add(LSTM(256, return_sequences=True))\n",
    "generator.add(Dropout(0.2))\n",
    "generator.add(Dense(input_dim, activation='softmax'))\n",
    "\n",
    "generator.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001, beta_1=0.5))\n",
    "\n",
    "# Define the training data generator\n",
    "def train_generator(batch_size):\n",
    "    while True:\n",
    "        random_indices = np.random.randint(0, len(smiles_list), batch_size)\n",
    "        random_smiles = [smiles_list[i] for i in random_indices]\n",
    "        x_train = np.array([smiles_to_vector(smiles) for smiles in random_smiles])\n",
    "        y_train = np.array([smiles_to_vector(smiles) for smiles in random_smiles])\n",
    "        yield x_train, y_train\n",
    "\n",
    "# Define the function to convert SMILES to vectors\n",
    "def smiles_to_vector(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=input_dim)\n",
    "    arr = np.zeros((1,))\n",
    "    Chem.DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "    return arr\n",
    "\n",
    "# Define the checkpoint to save the model during training\n",
    "#checkpoint = ModelCheckpoint('generator_model.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "# Train the generator model\n",
    "generator.fit_generator(train_generator(batch_size=32), steps_per_epoch=100, epochs=100, callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (default, Feb 28 2021, 17:03:44) \n[GCC 10.2.1 20210110]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
